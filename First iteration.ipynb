{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv3D, UpSampling3D, ReLU, Add, Flatten, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Directories for train and test data\n",
    "train_dir = \"path_to_train_data\"\n",
    "test_dir = \"path_to_test_data\"\n",
    "\n",
    "# Helper functions\n",
    "def load_training_data(base_dir, channel=\"NIR\", patch_size=(64, 64), stride=32):\n",
    "    \"\"\"Load training data (NIR or RED channel).\"\"\"\n",
    "    channel_dir = os.path.join(base_dir, channel)\n",
    "    lr_patches, hr_patches = [], []\n",
    "\n",
    "    for imgset in os.listdir(channel_dir):\n",
    "        imgset_path = os.path.join(channel_dir, imgset)\n",
    "        if not os.path.isdir(imgset_path):\n",
    "            continue\n",
    "\n",
    "        hr_path = os.path.join(imgset_path, \"HR.png\")\n",
    "        sm_path = os.path.join(imgset_path, \"SM.png\")\n",
    "        if not os.path.exists(hr_path) or not os.path.exists(sm_path):\n",
    "            continue\n",
    "\n",
    "        hr_image = cv2.imread(hr_path, cv2.IMREAD_GRAYSCALE)\n",
    "        status_map = cv2.imread(sm_path, cv2.IMREAD_GRAYSCALE)\n",
    "        hr_image = np.where(status_map == 1, hr_image, 0)\n",
    "\n",
    "        for file in os.listdir(imgset_path):\n",
    "            if file.startswith(\"LR\") and file.endswith(\".png\"):\n",
    "                lr_path = os.path.join(imgset_path, file)\n",
    "                qm_path = lr_path.replace(\"LR\", \"QM\")\n",
    "                if not os.path.exists(qm_path):\n",
    "                    continue\n",
    "\n",
    "                lr_image = cv2.imread(lr_path, cv2.IMREAD_GRAYSCALE)\n",
    "                quality_map = cv2.imread(qm_path, cv2.IMREAD_GRAYSCALE)\n",
    "                lr_image = np.where(quality_map == 1, lr_image, 0)\n",
    "\n",
    "                lr_patches.extend(extract_patches(lr_image, patch_size, stride))\n",
    "                hr_patches.extend(extract_patches(hr_image, patch_size, stride))\n",
    "\n",
    "    return np.array(lr_patches), np.array(hr_patches)\n",
    "\n",
    "def extract_patches(image, patch_size=(64, 64), stride=32):\n",
    "    \"\"\"Extract patches from an image.\"\"\"\n",
    "    patches = []\n",
    "    for i in range(0, image.shape[0] - patch_size[0] + 1, stride):\n",
    "        for j in range(0, image.shape[1] - patch_size[1] + 1, stride):\n",
    "            patch = image[i:i + patch_size[0], j:j + patch_size[1]]\n",
    "            patches.append(patch)\n",
    "    return patches\n",
    "\n",
    "def load_test_data(base_dir, channel=\"NIR\"):\n",
    "    \"\"\"Load test data (NIR or RED channel).\"\"\"\n",
    "    channel_dir = os.path.join(base_dir, channel)\n",
    "    test_images, status_maps = [], []\n",
    "\n",
    "    for imgset in os.listdir(channel_dir):\n",
    "        imgset_path = os.path.join(channel_dir, imgset)\n",
    "        if not os.path.isdir(imgset_path):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(imgset_path):\n",
    "            if file.startswith(\"LR\") and file.endswith(\".png\"):\n",
    "                lr_path = os.path.join(imgset_path, file)\n",
    "                qm_path = lr_path.replace(\"LR\", \"QM\")\n",
    "                if not os.path.exists(qm_path):\n",
    "                    continue\n",
    "\n",
    "                lr_image = cv2.imread(lr_path, cv2.IMREAD_GRAYSCALE)\n",
    "                status_map = cv2.imread(qm_path, cv2.IMREAD_GRAYSCALE)\n",
    "                test_images.append(lr_image)\n",
    "                status_maps.append(status_map)\n",
    "\n",
    "    return np.array(test_images), np.array(status_maps)\n",
    "\n",
    "# Load training data\n",
    "lr_nir_patches, hr_nir_patches = load_training_data(train_dir, channel=\"NIR\")\n",
    "lr_red_patches, hr_red_patches = load_training_data(train_dir, channel=\"RED\")\n",
    "lr_patches = np.concatenate([lr_nir_patches, lr_red_patches], axis=0)\n",
    "hr_patches = np.concatenate([hr_nir_patches, hr_red_patches], axis=0)\n",
    "lr_patches = lr_patches / 255.0\n",
    "hr_patches = hr_patches / 255.0\n",
    "\n",
    "# Load test data\n",
    "test_images, test_status_maps = load_test_data(test_dir, channel=\"NIR\")\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Generator Model\n",
    "def build_generator(input_shape=(None, 64, 64, 1)):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(inputs)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    for _ in range(16):\n",
    "        x_initial = x\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "        x = ReLU()(x)\n",
    "        x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "        x = Add()([x_initial, x])\n",
    "\n",
    "    x = UpSampling3D(size=(1, 2, 2))(x)\n",
    "    x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = UpSampling3D(size=(1, 2, 2))(x)\n",
    "    outputs = Conv3D(1, kernel_size=(3, 3, 3), padding='same', activation='tanh')(x)\n",
    "    return Model(inputs, outputs, name=\"Generator\")\n",
    "\n",
    "# Discriminator Model\n",
    "def build_discriminator(input_shape=(64, 64, 1)):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv3D(64, kernel_size=(3, 3, 3), padding='same')(inputs)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    for filters in [128, 256, 512]:\n",
    "        x = Conv3D(filters, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs, name=\"Discriminator\")\n",
    "\n",
    "# Loss Functions\n",
    "def content_loss(hr, sr):\n",
    "    return tf.reduce_mean(tf.square(hr - sr))\n",
    "\n",
    "def adversarial_loss(real_output, fake_output):\n",
    "    return -tf.reduce_mean(real_output) + tf.reduce_mean(fake_output)\n",
    "\n",
    "# Instantiate Models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(lr_batch, hr_batch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        sr_batch = generator(lr_batch, training=True)\n",
    "        real_output = discriminator(hr_batch, training=True)\n",
    "        fake_output = discriminator(sr_batch, training=True)\n",
    "\n",
    "        g_loss = content_loss(hr_batch, sr_batch) + adversarial_loss(real_output, fake_output)\n",
    "        d_loss = adversarial_loss(real_output, fake_output)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    disc_gradients = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    return g_loss, d_loss\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0, len(lr_patches), BATCH_SIZE):\n",
    "        lr_batch = lr_patches[i:i + BATCH_SIZE][..., np.newaxis]\n",
    "        hr_batch = hr_patches[i:i + BATCH_SIZE][..., np.newaxis]\n",
    "        g_loss, d_loss = train_step(lr_batch, hr_batch)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Gen Loss: {g_loss.numpy()}, Disc Loss: {d_loss.numpy()}\")\n",
    "\n",
    "# Evaluate Test Data\n",
    "def evaluate(generator, test_images, test_status_maps):\n",
    "    psnr_vals, ssim_vals = [], []\n",
    "    reconstructed_images = []\n",
    "\n",
    "    for lr_image, sm in zip(test_images, test_status_maps):\n",
    "        lr_image = lr_image[np.newaxis, ..., np.newaxis]  # Add batch and channel dimensions\n",
    "        sr_image = generator.predict(lr_image)[0, ..., 0]  # Remove batch and channel dimensions\n",
    "        sr_image = np.where(sm == 1, sr_image, 0)  # Mask invalid pixels\n",
    "        reconstructed_images.append(sr_image)\n",
    "\n",
    "        # Metrics\n",
    "        psnr_vals.append(psnr(sm, sr_image))\n",
    "        ssim_vals.append(ssim(sm, sr_image, multichannel=False))\n",
    "\n",
    "    print(f\"Average PSNR: {np.mean(psnr_vals):.2f}, Average SSIM: {np.mean(ssim_vals):.2f}\")\n",
    "    return reconstructed_images, psnr_vals, ssim_vals\n",
    "\n",
    "reconstructed_images, psnr_vals, ssim_vals = evaluate(generator, test_images, test_status_maps)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
